{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91257fc",
   "metadata": {},
   "source": [
    "# (Extract part) ETL: Cómo obtener todos los datos de todos los restaurantes en Rappi\n",
    "\n",
    "En síntesis, este notebook es el primero de 3 que explican cómo obtener en .csv:\n",
    "<ol>\n",
    "<li>Los países disponibles en Rappi: paises_rappi.csv</li>\n",
    "<li>Los cadenas de restaurantes disponibles en cada país: cadenas_restaurantes_df.csv</li>\n",
    "<li>Las sucursales de cada cadena junto con sus atributos.</li>\n",
    "<li>Una tabla aparte listando las opiniones ligadas a cada sucursal: opiniones_sucursales.csv</li>\n",
    "</ol>\n",
    "------\n",
    "Nota: Si decides ejecutar este código:\n",
    "<ol>\n",
    "<li>Instala las librerías listadas en requirements.txt</li>\n",
    "<li>Este código utiliza web scraping (Podría relantizar tu conexión a Internet.)</li>\n",
    "<li>Ten en cuenta que se usa multiprocessing para acelerar ciertas partes del ETL (Podría relantizar tu máquina.)</li>\n",
    "<li>Ligado al punto anterior, Estas mismas partes me demoraron entre 2 y 4 horas a terminar (Mi CPU tiene 4 cores y tengo una conexión de 230 megas/s).</li>\n",
    "</ol>\n",
    "<p>Nota personal: Aún falta enlistar bien las urls con error.</p>\n",
    "Fecha de edición: 4/12/2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c7b3a",
   "metadata": {},
   "source": [
    "## Obtenemos paises_rappi.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecd278",
   "metadata": {},
   "source": [
    "Importamos las librerías a utilizar durante esta parte del proyecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05aac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334964e",
   "metadata": {},
   "source": [
    "Debido a que usaremos scrapping con requests, definamos una pequeña función que corrobore si la conexión se logró o surgió un error y, de estar todo okey, nos retorne la sopa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_sopa(url):\n",
    "    url_request = requests.get(url)\n",
    "    if url_request.status_code == 200:\n",
    "        soup = BeautifulSoup(url_request.text, 'lxml')\n",
    "        return soup\n",
    "    else:\n",
    "        status = f'Hubo un problema con la url: {url}'\n",
    "        print(status)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c19bd4",
   "metadata": {},
   "source": [
    "A la fecha de esta edición, Rappi tiene todas las url's de cada país listadas en su footer. Con esto en mente obtendremos la etiqueta de los elementos que contienen los href's y los nombres de cada país."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef658a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos la url base\n",
    "url_main = \"https://www.rappi.com\"\n",
    "# Hacemos el request, verificamos status y obtenemos sopa\n",
    "sopa = obtener_sopa(url_main) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c655b25",
   "metadata": {},
   "source": [
    "Para listar todos los países de Rappi actualmente:\n",
    "1) Encontraremos el elemento con tag *ul* y id *1* que lista justo lo que necesitamos, a este le llamaremos *elemento_clave*.\n",
    "2) Luego obtendremos todos los elementos con etiqueta *a* dentro de *elemento_clave*. (En está etiqueta están las url's y los nombres de cada país)\n",
    "2) Luego listaremos cada url y cada nombre de cada país en *paises_url* y *paises_nombres*\n",
    "3) Imprimimos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d74d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "elemento_clave = sopa.find('ul', id=\"1\") # Primer paso\n",
    "elementos_a = elemento_clave.find_all('a') # Segundo paso\n",
    "paises_url = [elementos_a[i].get('href') for i in range(len(elementos_a))] # Tercer paso\n",
    "paises_nombres = [elementos_a[i].text for i in range(len(elementos_a))] # Sus nombres\n",
    "paises_url # Ya adivinaste jaajaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de707c3b",
   "metadata": {},
   "source": [
    "Listamos nuestros datos como df para exportarlo a .csv después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DF\n",
    "paises_df = pd.DataFrame({\"url_paises\": paises_url, \"nombre_pais\": paises_nombres})\n",
    "# Revisemos su estado\n",
    "paises_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3d194",
   "metadata": {},
   "source": [
    "Ahora, a la fecha de esta edición, listaremos el tipo de cambio en doláres para cada moneda latinoamericana. (Esto nos servirá para convertir los precios de cada catálogo en una sola currency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bf361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos la columna de intercambio de equivalencia a 1000 dólares.\n",
    "equivalente_10000_dolares = [\n",
    "    1676508,# Argentina\n",
    "    52195,# Brasil\n",
    "    8835000,# Chile\n",
    "    47680700,# Colombia\n",
    "    5981305, # Costa rica\n",
    "    10000, # Ecuador\n",
    "    193914,# Mexico\n",
    "    38223.49,# Perú\n",
    "    394049.80 # Uruguay\n",
    "]\n",
    "paises_df[\"1000_dollars_exchange\"] = equivalente_10000_dolares\n",
    "# Revisemos su estado\n",
    "paises_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06405d",
   "metadata": {},
   "source": [
    "Listo. Ahora solo exportamos y guardamos el .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_df.to_csv(\"paises_rappi.csv\", index_label=\"id_pais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d74c54",
   "metadata": {},
   "source": [
    "## Obteniendo cadenas_restaurantes_df.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b493b54",
   "metadata": {},
   "source": [
    "<p>Para nuestra suerte, Rappi ya ha indexado cada cadena de restaurantes de cada país en un solo catálogo. El mismo dividido en distintas páginas (de la A hasta la Z junto con las cadenas que empiezan con números).\n",
    "<p>Para acceder a este, solo se debe agregar \"/catalogo/restaurants/a-1\" a la url de cada país.\n",
    "<p>Ejemplo, para Ecuador sería: \"https://www.rappi.com.ec/catalogo/restaurants/a-a1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9eafe",
   "metadata": {},
   "source": [
    "Para lograr nuestro objetivo realizaremos 2 cosas:\n",
    "1) Listar todas las subpáginas de cada catálogo de cada país.\n",
    "2) Obtener los nombres y url's de cada cadena de cada subpágina.\n",
    "\n",
    "Para esto necesitaremos nuestro df de países."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536265d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paises_df = pd.read_csv(\"paises_rappi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9bff0",
   "metadata": {},
   "source": [
    "### Listar todas las subpáginas de cada catálogo de cada país."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c0799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nuestra url base\n",
    "url_inicial_catalogo = \"/catalogo/restaurants/a-1\"\n",
    "id_pais_subcatalogos_all = []\n",
    "urls_catalogos_all = []\n",
    "for i in paises_df.index:\n",
    "    url_pais_elegido = paises_df.url_paises[i][:-1] #Obtenemos la url de cada país quitando \n",
    "                                                    # -el último '/'\n",
    "    url_catalogo_pais = url_pais_elegido + url_inicial_catalogo\n",
    "    try:\n",
    "        catalogo_soup = obtener_sopa(url_catalogo_pais) # Obtenemos la sopa\n",
    "        if sopa != None: # Si no hubo ningun problema al obtener la sopa\n",
    "            # Obtenemos todos los subcatálogos\n",
    "            paginas_catalogo = catalogo_soup.find_all(class_=\"sc-39328323-1 jiuMaW\")\n",
    "            # Obtenemos cada url de cada subcatálogo y lo concatenamos todo\n",
    "            urls_subcatalogos = [url_pais_elegido + paginas_catalogo[j].get(\"href\")\n",
    "                                  for j\n",
    "                                  in range(len(paginas_catalogo))]\n",
    "            urls_catalogos_all = urls_catalogos_all + urls_subcatalogos\n",
    "            # Igualmente con el id del país correspondiente\n",
    "            id_pais_subcatalogos_all = id_pais_subcatalogos_all + [i\n",
    "                                                               for j\n",
    "                                                               in range(len(urls_subcatalogos))]\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Porcentaje completado: {round((i+1)*100 / len(paises_df.index), 2)}\")\n",
    "    except:\n",
    "        print(f\"No se pudo obtener el catalogo de: {url_catalogo_pais}\")\n",
    "        continue\n",
    "print(\"Terminado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45712678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DF\n",
    "subcatalogos_df = pd.DataFrame({\"url_subcatalogos\": urls_catalogos_all, \"id_pais\": id_pais_subcatalogos_all})\n",
    "# Revisemos su estado\n",
    "subcatalogos_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d355e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcatalogos_df.to_csv(\"subcatalogos.csv\",index_label=\"id_subcatalogo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32992eb9",
   "metadata": {},
   "source": [
    "### Obtener los nombres y url's de cada cadena de cada subpágina.\n",
    "Para esto usaremos las url's obtenidas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06574d9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subcatalogos_df = pd.read_csv(\"subcatalogos.csv\")\n",
    "# Revisamos su estado\n",
    "subcatalogos_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5514b8",
   "metadata": {},
   "source": [
    "<p>En la interfaz encontramos que las *urls* y los *nombres* de los restaurantes podemos obtenerlos mediante la clase \"sc-bdfBQB eXopiF sc-iqHYmW gcZftM secondary\". Con esto en mente corremos el siguiente código:</p>\n",
    "---------\n",
    "<p>Nota: A partir de aquí utilizaremos multiprocessing para acelerar las cosas, de modo que dejaremos de agrupar nuestros datos en columnas sino en filas</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72df6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "problema_url = []\n",
    "def proccess_subcatalogos(index_to_process):\n",
    "    i = index_to_process\n",
    "    url_cadena_resturantes = []\n",
    "    nombre_cadena_resturantes = []\n",
    "    id_pais = []\n",
    "    \n",
    "    url_subcatalogo = subcatalogos_df.url_subcatalogos[i] # Obtenemos la url\n",
    "    sub_id_pais = subcatalogos_df.id_pais[i] # Además del id del país\n",
    "    sopa = obtener_sopa(url_subcatalogo) # Obtenemos la sopa\n",
    "    # Trabajamos la sopa\n",
    "    try:\n",
    "        if sopa != None: # Si no hubo ningun problema al obtener la sopa\n",
    "            # Obtenemos todos los nombres y urls y las concatenamos\n",
    "            elementos_cadena_resturantes = sopa.find_all(class_=\"sc-bdfBQB eXopiF sc-iqHYmW gcZftM secondary\")\n",
    "            url_cadena_resturantes = url_cadena_resturantes + [elementos_cadena_resturantes[j].get(\"href\")\n",
    "                                                              for j\n",
    "                                                              in range(len(elementos_cadena_resturantes))]\n",
    "            nombre_cadena_resturantes = nombre_cadena_resturantes + [elementos_cadena_resturantes[j].find_all(\"span\")[0].text\n",
    "                                                                     for j \n",
    "                                                                    in range(len(elementos_cadena_resturantes))]\n",
    "            # Luego guardamos el id del país correspondiente\n",
    "            id_pais = id_pais + [sub_id_pais\n",
    "                               for j\n",
    "                               in range(len(nombre_cadena_resturantes))]\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Porcentaje completado: {round((i+1)*100 / len(subcatalogos_df.index), 2)}\")\n",
    "    except:\n",
    "        print(f\"No se pudo las sucursales en: {url_subcatalogo}\")\n",
    "        problema_url.append(url_subcatalogo)\n",
    "    return (url_cadena_resturantes, nombre_cadena_resturantes, id_pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cf046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí usamos concurrent y multiproccesing para acelerar nuestro tiempo\n",
    "all_index_subcatalogos = subcatalogos_df.index\n",
    "# Obtenemos los datos\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(proccess_subcatalogos, all_index_subcatalogos)\n",
    "# Guardamos nuestra data\n",
    "data = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890806c",
   "metadata": {},
   "source": [
    "Revisamos si obtuvimos algún error en alguna url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "problema_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53de82b",
   "metadata": {},
   "source": [
    "Cool, ninguno. Ahora nuestra series están guardadas en subtuplas de sublistas de la lista data. Para poder utilizarla tenemos que descomprimir estas tuplas y luego concatenarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf76bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos pd.Series vacías para usarlas en la concatenación\n",
    "url_cadena_resturantes = pd.Series([], dtype=str)\n",
    "nombre_cadena_resturantes = pd.Series([], dtype=str)\n",
    "id_pais = pd.Series([], dtype=\"int64\")\n",
    "\n",
    "# Concatenamos cada lista con su respectiva serie\n",
    "for tuplita in data:\n",
    "    url_cadena_resturantes_next = pd.Series(tuplita[0])\n",
    "    url_cadena_resturantes = pd.concat([url_cadena_resturantes_next,url_cadena_resturantes])\n",
    "    \n",
    "    nombre_cadena_resturantes_next = pd.Series(tuplita[1])\n",
    "    nombre_cadena_resturantes = pd.concat([nombre_cadena_resturantes_next,nombre_cadena_resturantes])\n",
    "    \n",
    "    id_pais_next = pd.Series(tuplita[2])\n",
    "    id_pais = pd.concat([id_pais_next,id_pais])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0961ca",
   "metadata": {},
   "source": [
    "Cool, todo okey. Ahora veamos como quedo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ac184",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadenas_restaurantes_df = pd.DataFrame({\"nombre_cadena\": nombre_cadena_resturantes,\n",
    "                                       \"url_cadena\": url_cadena_resturantes,\n",
    "                                       \"id_pais\": id_pais})\n",
    "# Lo revisamos\n",
    "cadenas_restaurantes_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseteamos el índice\n",
    "cadenas_restaurantes_df.reset_index(drop=True, inplace=True)\n",
    "# Veamos como queda finalmente\n",
    "cadenas_restaurantes_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb05f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos nuestro trabajo\n",
    "cadenas_restaurantes_df.to_csv(\"cadenas_restaurantes.csv\", index_label=\"id_cadena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379944f6",
   "metadata": {},
   "source": [
    "## Sucursales: Obteniendo todas las sucursales de cada cadena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48434324",
   "metadata": {},
   "source": [
    "Cool, a 1 paso de completar nuestra extracción. Ahora, en cada cadena de restaurantes existen diferentes sucursales y cada una de ellas con un nombre, una url y una dirección únicas).\n",
    "Ahora tocaría:\n",
    "1) Aplicar la misma idea del paso anterior pero para cada sucursal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos nuestro trabajo anterior\n",
    "cadenas_restaurantes_df = pd.read_csv(\"cadenas_restaurantes.csv\") \n",
    "# Lo revisamos\n",
    "cadenas_restaurantes_df.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bcc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "problema_url = []\n",
    "no_hay_sucursales_url = []\n",
    "def proccess_sucursales(index_to_process):\n",
    "    i = index_to_process\n",
    "    url_sucursales = []\n",
    "    nombre_sucursales = []\n",
    "    direccion_sucursales = []\n",
    "    id_cadena = []\n",
    "    \n",
    "    url_cadena_elegida = cadenas_restaurantes_df.url_cadena[i] # Obtenemos la url\n",
    "    sub_id_cadena = cadenas_restaurantes_df.id_cadena[i] # Además del id del país\n",
    "    sopa = obtener_sopa(url_cadena_elegida) # Obtenemos la sopa\n",
    "    # Trabajamos la sopa\n",
    "    try:\n",
    "        if sopa != None: # Si no hubo ningun problema al obtener la sopa\n",
    "            div_sucursales = sopa.find(\"div\",\n",
    "                              {'data-testid': 'topRestCard'},\n",
    "                              class_=\"sc-9fb51c13-6 fIgfiC\"\n",
    "                              )\n",
    "            try:\n",
    "                sucursales = div_sucursales.find_all(\"a\")\n",
    "                # Obtenemos todos los nombres y urls y las concatenamos\n",
    "                url_pais_base = re.split(\"/restaurantes\" , url_cadena_elegida)[0]\n",
    "                # Obtenemos los atributos de cada sucursal\n",
    "                url_sucursales = url_sucursales + [url_pais_base + sucursales[i].get(\"href\")\n",
    "                                                   for i\n",
    "                                                   in range(len(sucursales))]\n",
    "                nombre_sucursales = nombre_sucursales + [sucursales[i].find_all(\"h3\")[0].text\n",
    "                                                        for i\n",
    "                                                         in range(len(sucursales))]\n",
    "                direccion_sucursales = direccion_sucursales + [sucursales[i].find_all(\"div\", class_=\"sc-bxivhb fFeDyp sc-d9669f19-7 iOdleX\")[0].text\n",
    "                                                               for i\n",
    "                                                               in range(len(sucursales))]\n",
    "                # Luego guardamos el id del país correspondiente\n",
    "                id_cadena = id_cadena + [sub_id_cadena\n",
    "                                   for j\n",
    "                                   in range(len(nombre_sucursales))]\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Porcentaje completado: {round((i+1)*100 / len(cadenas_restaurantes_df.index), 2)}\")\n",
    "            except:\n",
    "                print(f\"No hay sucursales en la url {url_cadena_elegida}.\")\n",
    "                no_hay_sucursales_url.append(url_cadena_elegida)\n",
    "    except:\n",
    "        print(f\"No se pudo ingresar a: {url_cadena_elegida}\")\n",
    "        problema_url.append(url_cadena_elegida)\n",
    "    return (url_sucursales, nombre_sucursales, direccion_sucursales, id_cadena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9370aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aplicamos multiproccesing\n",
    "all_index_subcatalogos = cadenas_restaurantes_df.index\n",
    "# Obtenemos los datos\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(proccess_sucursales, all_index_subcatalogos)\n",
    "# Guardamos nuestra data\n",
    "data = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3aad5c",
   "metadata": {},
   "source": [
    "Revisamos si obtuvimos algún error en alguna url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Problemas encontrados urls: {len(problema_url)}\")\n",
    "print(f\"Cadenas sin sucursal: {len(no_hay_sucursales_url)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340a8fc",
   "metadata": {},
   "source": [
    "En este caso nos lista que no hay errores, esto sucede porque no he puesto las variables en global de modo que sucede esto. Pero como pudimos ver en el Output, sí hay cadenas que o no tienen sucursales disponibles o no se pudo acceder a estas mismas. Por el momento lo dejaremos pasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedc776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos pd.Series vacías para usarlas en la concatenación\n",
    "url_sucursal = pd.Series([], dtype=str)\n",
    "nombre_sucursal = pd.Series([], dtype=str)\n",
    "direccion_sucursal = pd.Series([], dtype=str)\n",
    "id_cadena = pd.Series([], dtype=\"int64\")\n",
    "\n",
    "# Concatenamos cada lista con su respectiva serie\n",
    "for i, tuplita in enumerate(data):\n",
    "    url_sucursal_next = pd.Series(tuplita[0], dtype=str)\n",
    "    url_sucursal = pd.concat([url_sucursal_next,url_sucursal])\n",
    "    \n",
    "    nombre_sucursal_next = pd.Series(tuplita[1], dtype=str)\n",
    "    nombre_sucursal = pd.concat([nombre_sucursal_next,nombre_sucursal])\n",
    "    \n",
    "    direccion_sucursal_next = pd.Series(tuplita[2], dtype=str)\n",
    "    direccion_sucursal = pd.concat([direccion_sucursal_next, direccion_sucursal])\n",
    "    \n",
    "    id_cadena_next = pd.Series(tuplita[3], dtype=\"int64\")\n",
    "    id_cadena = pd.concat([id_cadena_next,id_cadena])\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Porcentaje completado: {round((i+1)*100 / len(data), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259771ea",
   "metadata": {},
   "source": [
    "Bueno, como podrás imaginarte, para este proposito también hay una clase y una etiqueta específica que revisar jaja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "sucursales_df = pd.DataFrame({\"id_cadena\": id_cadena,\n",
    "                              \"url_sucursal\": url_sucursal,\n",
    "                              \"nombre_sucursal\": nombre_sucursal,\n",
    "                             \"direccion_sucursal\": direccion_sucursal})\n",
    "# Lo revisamos\n",
    "sucursales_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da267576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ordenamos en base a la id_cadena\n",
    "sucursales_df = sucursales_df.sort_values(\"id_cadena\", axis=0)\n",
    "# Reseteamos el índice\n",
    "sucursales_df.reset_index(drop=True, inplace=True)\n",
    "# Veamos como queda finalmente\n",
    "sucursales_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abbb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos nuestro trabajo\n",
    "sucursales_df.to_csv(\"sucursales.csv\", index_label=\"id_sucursal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915131f5",
   "metadata": {},
   "source": [
    "Cool, todo bien. La única nota ha dejar es que existen algunas cadenas de comida a las que no se pudieron sacar sus sucursales. Esto se debe a que SÍ están indexadas en el directorio de Rappi pero NO tienen sucursales disponibles, se han dado de baja o por x razón no aparecen sus sucursales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94b576",
   "metadata": {},
   "source": [
    "## Obteniendo las opiniones, precios y otros atributos de cada sucursal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a55153",
   "metadata": {},
   "source": [
    "Con esto ingresaremos a cada sucursal y obtendremos la siguiente información:\n",
    "- Opiniones: Las opiniones y sus porcentajes de cada sucursal.\n",
    "- Precios: Todos los precios de cada producto listado en el catálogo.\n",
    "- Otros atributos: Número de calificaciones, estrellas promedio, tiempo de envío, etc.\n",
    "----\n",
    "En algunas no se tienen estos datos, así que tendremos que poner \"No abierto al público / No calificado\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28821e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sucursales_df = pd.read_csv(\"sucursales.csv\")\n",
    "sucursales_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d6780",
   "metadata": {},
   "source": [
    "Ahora obtenemos los datos que nos interesan. Probaremos con una sucursal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos la url de la sucursal de ejemplo\n",
    "sucursal_url = sucursales_df.url_sucursal[3]\n",
    "print(f\"Obteniendo sopa de: {sucursal_url}\")\n",
    "# Obtenemos la sopa\n",
    "sopa = obtener_sopa(sucursal_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb82063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos una lambda para retornar \"None\" si no hay opiniones o las opiniones si sí las hay.\n",
    "revisador_de_nonetypes = lambda element: np.nan if element == None else element.text\n",
    "# Creamos nuestra función para obtener los atributos\n",
    "def get_all_attr(soup_sucursal):\n",
    "    try:\n",
    "        # Obtenemos las estrellas y el número de opiniones\n",
    "        overral_stars = revisador_de_nonetypes(soup_sucursal.find(\"span\", class_=\"sc-bxivhb gJCKbU\"))\n",
    "        number_opinions = revisador_de_nonetypes(soup_sucursal.find(\"span\", class_=\"sc-bxivhb dVvqfA\")) # Le quitamos los parentesis\n",
    "        tiempo_delivery = revisador_de_nonetypes(soup_sucursal.find(\"span\", class_=\"sc-bxivhb ecrUmJ\"))\n",
    "        tipo_envio = revisador_de_nonetypes(soup_sucursal.find(\"div\", class_=\"chakra-skeleton css-1vjr0v9\"))\n",
    "        if not pd.isna(number_opinions):\n",
    "            number_opinions = int(number_opinions[1:-1]) # Para quitar los parentesis y obtener el número\n",
    "        return [overral_stars, number_opinions, tiempo_delivery, tipo_envio]\n",
    "    except:\n",
    "        print(\"03 Problema al obtener atributos globales. Retornando error\")\n",
    "        error = \"Problema atributos\"\n",
    "        return error\n",
    "get_all_attr(sopa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dd24c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nuestra función para obtener todos los precios\n",
    "def get_all_prices(soup_sucursal):\n",
    "    try:\n",
    "        # Encontrar lista de precios de productos\n",
    "        div_prices_unparsed = soup_sucursal.find_all(\"span\", class_=\"chakra-text css-kowr8\")\n",
    "        prices = [div_prices_unparsed[i].text\n",
    "                        for i in range(len(div_prices_unparsed))]\n",
    "        return prices\n",
    "    except:\n",
    "        print(\"02 Problema al obtener precios. Retornando error\")\n",
    "        prices = \"Problema precios\"\n",
    "        return prices\n",
    "get_all_prices(sopa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3714d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuestra sopa para obtener las diferentes opiniones con su porcentaje\n",
    "def get_all_opinions(soup_sucursal):\n",
    "    try:\n",
    "        div_opinions_unparsed = soup_sucursal.find_all(\"div\", class_=\"css-z7mtfw\")\n",
    "        # Nos quedamos con los spans de cada opinion para tener su texto y porcentajes\n",
    "        opinions_unparsed = [div_opinions_unparsed[i].find_all(\"span\")\n",
    "                        for i in range(len(div_opinions_unparsed))]\n",
    "        # Agarramos el texto Y porcentajes\n",
    "        opinions = [(opinions_unparsed[i][0].text, opinions_unparsed[i][1].text)\n",
    "                    for i in range(len(opinions_unparsed))]\n",
    "        return opinions\n",
    "    except:\n",
    "        print(\"01 Problema al obtener opiniones. Retornando error\")\n",
    "        opinions = \"Error opiniones\"\n",
    "        return opinions\n",
    "get_all_opinions(sopa) # Si retorna una lista vacía es que no se encontraron opiniones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96910066",
   "metadata": {},
   "source": [
    "Con estos datos podemos empezar a obtener todos los metadatos de cada sucursal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7db1bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "problema_obtener_url = []\n",
    "# Definición de nuestra función base\n",
    "def process_sucursal(index_sucursal):\n",
    "    i = index_sucursal\n",
    "    sucursal_url = sucursales_df.url_sucursal[i]\n",
    "    # Obtenemos el ID de la sucursal\n",
    "    id_sucursal = sucursales_df.id_sucursal[i] \n",
    "    try:\n",
    "        # Obtenemos la sopa\n",
    "        sopa = obtener_sopa(sucursal_url)\n",
    "        attributes = get_all_attr(sopa)\n",
    "        prices = get_all_prices(sopa)\n",
    "        opinions = get_all_opinions(sopa)\n",
    "        print(f\"Porcentaje completado: {round((i+1)*100 / len(sucursales_df.index), 2)}\")\n",
    "        return id_sucursal, attributes, prices, opinions\n",
    "    except:\n",
    "        try:\n",
    "            # Obtenemos la sopa\n",
    "            print(\"Intentando obtener sopa denuevo\")\n",
    "            sopa = obtener_sopa(sucursal_url)\n",
    "            # Obtenemos todos los datos\n",
    "            attributes = get_all_attr(sopa)\n",
    "            prices = get_all_prices(sopa)\n",
    "            opinions = get_all_opinions(sopa)\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Porcentaje completado: {round((i+1)*100 / len(sucursales_df.index), 2)}\")\n",
    "            return id_sucursal, attributes, prices, opinions\n",
    "        except:\n",
    "            print(f\"Hubo un problema con la sopa de la url {url_sucursal_elegida}\")\n",
    "            problema_obtener_url.append(url_sucursal_elegida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a0be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aquí usamos concurrent y multiproccesing para acelerar nuestro tiempo\n",
    "all_index_sucursales = sucursales_df.index\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(process_sucursal, all_index_sucursales)\n",
    "    \n",
    "# Guardamos la data\n",
    "data = list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bc643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo ponemos en un df\n",
    "atributos_sucursales_bruto_df = pd.DataFrame.from_records(data,\n",
    "                                                    columns = [\"id_sucursal\",\n",
    "                                                               \"attributes\",\n",
    "                                                               \"prices\",\n",
    "                                                               \"opinions\"])\n",
    "# Revisemos que tal quedó\n",
    "atributos_sucursales_bruto_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da628f5",
   "metadata": {},
   "source": [
    "## Revisión de últimos errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e4b717",
   "metadata": {},
   "source": [
    "Busquemos si queda algún error por allí. Por si acaso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_sucursales_bruto_df[atributos_sucursales_bruto_df[\"opinions\"] == \"Error opiniones\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59972a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna la lista de índices en donde se detectaron errores\n",
    "revisar = atributos_sucursales_bruto_df[atributos_sucursales_bruto_df[\"opinions\"] == \"Error opiniones\"].index\n",
    "revisar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14984129",
   "metadata": {},
   "source": [
    "Veamos si podemos solucionarlo aplicando la función denuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dba718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reapliquemos \n",
    "all_index_sucursales = revisar\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(process_sucursal, all_index_sucursales)\n",
    "    \n",
    "# Guardamos la data\n",
    "data = list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a86d95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Revisemos\n",
    "atributos_sucursales_bruto_df_errores = pd.DataFrame.from_records(data,\n",
    "                                                    columns = [\"id_sucursal\",\n",
    "                                                               \"attributes\",\n",
    "                                                               \"prices\",\n",
    "                                                               \"opinions\"])\n",
    "# Revisemos que tal quedó\n",
    "atributos_sucursales_bruto_df_errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c2cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna la lista de índices en donde se detectaron errores (denuevo)\n",
    "revisar_errores = atributos_sucursales_bruto_df_errores[atributos_sucursales_bruto_df_errores[\"opinions\"] == \"Error opiniones\"].index\n",
    "len(revisar_errores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da39e19",
   "metadata": {},
   "source": [
    "Bien, parece que nos decisimos de los deshicimos de los errores. Reemplazemos los nuevos valores nuevos por los anteriores malos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recambiemos con los datos buenos por los malos en el df original\n",
    "for i, j in enumerate(atributos_sucursales_bruto_df_errores.id_sucursal):\n",
    "    new_data = atributos_sucursales_bruto_df_errores[atributos_sucursales_bruto_df_errores[\"id_sucursal\"] == j].loc[i,:]\n",
    "    atributos_sucursales_bruto_df.iloc[j,:] = new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos que todo haya sido cambiado\n",
    "atributos_sucursales_bruto_df.loc[revisar,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e18992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos por errores una última vez\n",
    "revisar = atributos_sucursales_bruto_df[atributos_sucursales_bruto_df[\"opinions\"] == \"Error opiniones\"].index\n",
    "len(revisar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab54643",
   "metadata": {},
   "source": [
    "Cool, con esto hecho, ordenamos nuestro df y lo envíamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenamos el df\n",
    "atributos_sucursales_bruto_df = atributos_sucursales_bruto_df.sort_values(\"id_sucursal\")\n",
    "# Lo guardamos\n",
    "atributos_sucursales_bruto_df.to_csv(\"atributos_sucursales_bruto.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abecf9",
   "metadata": {},
   "source": [
    "Listo. Hasta aquí acabaría la en E en nuestro ETL, continua a la siguiente parte 'Transform' para conocer como seguimos desde aquí. Gracias por tu tiempo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b590a1a9dbce2500618ab1c29bb116cf2b8d3ff87d2d3946a9e6de6a2d869cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
